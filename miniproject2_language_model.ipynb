{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--Cvru1cgwyP"
   },
   "source": [
    "# **Miniproject 2**\n",
    "## **~Large~ Small Language Model**\n",
    "\n",
    "# Paridhi Lohani\n",
    "\n",
    "### **Objective**\n",
    "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
    "\n",
    "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_rT3xwrhieb"
   },
   "source": [
    "### **Dataset**:\n",
    "\n",
    "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
    "\n",
    "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
    "\n",
    "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
    "\n",
    "An interface for the dataset class that takes care of tokenization is provided below.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV7OAXGRhf_V"
   },
   "source": [
    "### **Requirements**\n",
    "\n",
    "#### **Architecture**\n",
    "\n",
    "Implement the Transformer's decoder-only structure.\n",
    "This includes\n",
    "\n",
    "* input token embeddings\n",
    "* the causal multi-head self-attention mechanism\n",
    "* feed-forward neural networks\n",
    "* positional encodings, residual connections, layer normalizations.\n",
    "\n",
    "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
    "\n",
    "The `forward` method for the entire model has the following form:\n",
    "\n",
    "```\n",
    "tok_emb = WTE(idx) # token embeddings\n",
    "pos_emb = WPE(pos) # position embeddings\n",
    "x = Dropout(tok_emb + pos_emb)\n",
    "for Block in Blocks:\n",
    "    x = Block(x)\n",
    "x = Final_LayerNorm(x)\n",
    "logits = LM_Head(x)\n",
    "```\n",
    "\n",
    "The `forward` method for the transformer block has the following form:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
    "out = x + self.MLP(self.LayerNorm_2(x))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Training**\n",
    "\n",
    "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
    "\n",
    "Preprocess the dataset to a character-level representation.\n",
    "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
    "Implement causal masking for the self-attention mechanism.\n",
    "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
    "\n",
    "**Optional**:\n",
    "\n",
    "* Implement a learning rate decay strategy\n",
    "* Implement gradient clipping\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **Evaluation and Inference**\n",
    "\n",
    "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
    "\n",
    "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
    "\n",
    "The high-level pseudocode for generation is:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    context = \"O God, O God!\"\n",
    "    tokenized_context = tokenize(context)\n",
    "    # the model should implement a method to generate tokens given a prompt\n",
    "    y = model.generate(tokenized, ...)\n",
    "    completion = tokens_to_string(y)\n",
    "```\n",
    "\n",
    "**Optional**:\n",
    "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t88Dcn8JZ8M"
   },
   "source": [
    "### **Example Outputs**\n",
    "\n",
    "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "O God, O God! neither? unto the base very ears,\n",
    "As damned with it.\n",
    "\n",
    "DUKE OF YORK:\n",
    "Away! Once more, one word.\n",
    "\n",
    "RICHARD:\n",
    "Clove, dear so; and therein my son will be\n",
    "false of woe: if ye seems to be the mother\n",
    "Of gracious order this time when R going kinsperse eyes,\n",
    "What dost bewreck her fairer drying tears.\n",
    "\n",
    "NORTHUMBERLAND:\n",
    "Have you forgot the Duke of Norfolk, get him to\n",
    "again; and and agilic: there is my spirit\n",
    "So maly did must such a marble perfection.\n",
    "\n",
    "ELBOW:\n",
    "Come, bring them with oaths, and so deliver\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0SY7CGAhnkp"
   },
   "source": [
    "### Resources:\n",
    "\n",
    "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dZdSRWPmgt-H"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "#%pip install torch\n",
    "import urllib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = urllib.request.urlopen(url)\n",
    "corpus = response.read().decode('utf-8')\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        chars = sorted(set(data)) # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.data = data\n",
    "        self.block_size = config[1] #where config is of type (B,N) where B is batch size and N is block_size\n",
    "        self.batch_size = config[0]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data)) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        chunk = self.data[idx:idx+self.block_size+1]\n",
    "        int_chunk = [self.stoi[c] for c in chunk]\n",
    "        input_seq = int_chunk[:-1]\n",
    "        target_seq = int_chunk[1:]\n",
    "\n",
    "        return (torch.tensor(input_seq),torch.tensor(target_seq))\n",
    "\n",
    "    def get_batch(self):\n",
    "        ix = torch.randint(len(self),(self.batch_size,))\n",
    "        xys = [self.__getitem__(i) for i in ix]\n",
    "        x = torch.stack([xy[0] for xy in xys])\n",
    "        y = torch.stack([xy[1] for xy in xys])\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        B,T,C = inp.shape\n",
    "        k = self.key(inp)\n",
    "        q = self.query(inp)\n",
    "\n",
    "        attention = q@k.transpose(-2,-1) * k.shape[-1]**-0.5        \n",
    "        tril = torch.tril(torch.ones(T, T)).to(inp.device) \n",
    "        attention = attention.masked_fill(tril == 0,float('-inf'))\n",
    "        attention = F.softmax(attention,dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        return (attention @ self.value(inp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 8, 32])\n",
      "Output shape: torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32\n",
    "head_size = 16\n",
    "block_size = 10\n",
    "dropout = 0.2\n",
    "batch_size = 4\n",
    "sequence_length = 8\n",
    "mock_input = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "\n",
    "head = Head(head_size=head_size)\n",
    "\n",
    "output = head(mock_input)\n",
    "\n",
    "print(\"Input shape:\", mock_input.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionMechanism(nn.Module):\n",
    "    def __init__(self,num_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for i in range(num_heads)])\n",
    "        self.projected = nn.Linear(head_size*num_heads,embed_dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        output = torch.cat([h(inp) for h in self.heads],dim =-1)\n",
    "        return self.dropout(self.projected(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "mha = MultiHeadAttentionMechanism(num_heads = 2 , head_size = head_size)\n",
    "x = torch.randn(batch_size,sequence_length,embed_dim)\n",
    "output = mha(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim,4*embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*embed_dim,embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,inp):\n",
    "        return self.net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.008808970451355\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ffn = FeedForwardNetwork(embed_dim).to(device)\n",
    "inp = torch.randn(batch_size,sequence_length,embed_dim)\n",
    "output = ffn.forward(inp)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "target = torch.randn_like(output)\n",
    "loss = loss_fn(output, target)\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_dim,n_head):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim//n_head\n",
    "        self.ffn = FeedForwardNetwork(embed_dim)\n",
    "        self.layer_Norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_Norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.selfAttn = MultiHeadAttentionMechanism(n_head,head_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.selfAttn(self.layer_Norm1(x))\n",
    "        x = x + self.ffn(self.layer_Norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.0736894607543945\n"
     ]
    }
   ],
   "source": [
    "tb = TransformerBlock(embed_dim,n_head=2)\n",
    "inp = torch.randn(batch_size,sequence_length,embed_dim)\n",
    "output = tb.forward(inp)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "target = torch.randn_like(output)\n",
    "loss = loss_fn(output, target)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.WTE = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.WPE = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embed_dim,n_head) for i in range(n_layer)])\n",
    "        self.Final_LayerNorm = nn.LayerNorm(embed_dim)\n",
    "        self.LM_Head = nn.Linear(embed_dim,vocab_size)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.apply(self.__init_weights_)\n",
    "\n",
    "    def __init_weights_(self, module):\n",
    "        if isinstance(module, nn.Linear):  # Initialize weights for Linear layers\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.uniform_(module.weight, -0.1, 0.1)\n",
    "        elif isinstance(module, nn.LayerNorm): \n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        tok_emb = self.WTE(idx) # token embeddings\n",
    "\n",
    "        pos = torch.arange(idx.size(1), device=idx.device).unsqueeze(0).expand(idx.size(0), -1)\n",
    "        pos_emb = self.WPE(pos)\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        x = self.Dropout(tok_emb + pos_emb)\n",
    "        for Block in self.blocks:\n",
    "            x = Block(x)\n",
    "        x = self.Final_LayerNorm(x)\n",
    "        logits = self.LM_Head(x)\n",
    "        loss = None\n",
    "        \n",
    "        if targets is not None: #calculating cross entropy loss when there are targets\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        self.eval()\n",
    "        idx = idx.to(self.device)\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = idx[:,-block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx = torch.cat((idx,idx_next),dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (32,64)\n",
    "batch_size = config[0]\n",
    "block_size = config[1]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embed_dim = 96\n",
    "n_head = 3\n",
    "n_layer = 3\n",
    "dropout = 0.3\n",
    "max_iters = 1000\n",
    "eval_iters = 200\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "61\n",
      "1115330\n",
      "1003790\n",
      "111476\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "split = int(0.9*len(corpus))\n",
    "train_data = corpus[:split]\n",
    "validation_data = corpus[split:]\n",
    "\n",
    "CD_corpus = CharDataset(config,corpus)\n",
    "CD_train = CharDataset(config,train_data)\n",
    "CD_test = CharDataset(config,validation_data)\n",
    "\n",
    "vocab_size = CD_corpus.get_vocab_size()\n",
    "print(vocab_size)\n",
    "print(CD_test.get_vocab_size())\n",
    "print(len(CD_corpus))\n",
    "print(len(CD_train))\n",
    "print(len(CD_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split_type, cd in [('train', CD_train), ('val', CD_test)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = cd.get_batch()\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split_type] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShakespeareGPT()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 of 5 and Step: 1 of 5000 train loss: 4.3593058586120605 val loss 4.5930914878845215\n",
      "Epoch: 1 of 5 and Step: 101 of 5000 train loss: 3.2708704471588135 val loss 4.258737087249756\n",
      "Epoch: 1 of 5 and Step: 201 of 5000 train loss: 2.9573352336883545 val loss 4.456110000610352\n",
      "Epoch: 1 of 5 and Step: 301 of 5000 train loss: 2.7283153533935547 val loss 4.545886516571045\n",
      "Epoch: 1 of 5 and Step: 401 of 5000 train loss: 2.662909746170044 val loss 4.690065383911133\n",
      "Epoch: 1 of 5 and Step: 501 of 5000 train loss: 2.6230576038360596 val loss 4.739013195037842\n",
      "Epoch: 1 of 5 and Step: 601 of 5000 train loss: 2.5968363285064697 val loss 4.845954418182373\n",
      "Epoch: 1 of 5 and Step: 701 of 5000 train loss: 2.5708084106445312 val loss 4.894919395446777\n",
      "Epoch: 1 of 5 and Step: 801 of 5000 train loss: 2.5539159774780273 val loss 4.948029518127441\n",
      "Epoch: 1 of 5 and Step: 901 of 5000 train loss: 2.533412218093872 val loss 4.991334915161133\n",
      "Epoch: 2 of 5 and Step: 1001 of 5000 train loss: 2.5214037895202637 val loss 5.024808406829834\n",
      "Epoch: 2 of 5 and Step: 1101 of 5000 train loss: 2.508620023727417 val loss 5.079161643981934\n",
      "Epoch: 2 of 5 and Step: 1201 of 5000 train loss: 2.4915664196014404 val loss 5.1222991943359375\n",
      "Epoch: 2 of 5 and Step: 1301 of 5000 train loss: 2.478790521621704 val loss 5.1250386238098145\n",
      "Epoch: 2 of 5 and Step: 1401 of 5000 train loss: 2.467405319213867 val loss 5.187249183654785\n",
      "Epoch: 2 of 5 and Step: 1501 of 5000 train loss: 2.464503288269043 val loss 5.2512640953063965\n",
      "Epoch: 2 of 5 and Step: 1601 of 5000 train loss: 2.4539639949798584 val loss 5.269697189331055\n",
      "Epoch: 2 of 5 and Step: 1701 of 5000 train loss: 2.440659523010254 val loss 5.289999961853027\n",
      "Epoch: 2 of 5 and Step: 1801 of 5000 train loss: 2.428701877593994 val loss 5.321901798248291\n",
      "Epoch: 2 of 5 and Step: 1901 of 5000 train loss: 2.426579236984253 val loss 5.3763933181762695\n",
      "Epoch: 3 of 5 and Step: 2001 of 5000 train loss: 2.4196126461029053 val loss 5.366058349609375\n",
      "Epoch: 3 of 5 and Step: 2101 of 5000 train loss: 2.404771327972412 val loss 5.397927761077881\n",
      "Epoch: 3 of 5 and Step: 2201 of 5000 train loss: 2.392242431640625 val loss 5.431969165802002\n",
      "Epoch: 3 of 5 and Step: 2301 of 5000 train loss: 2.384056329727173 val loss 5.441630840301514\n",
      "Epoch: 3 of 5 and Step: 2401 of 5000 train loss: 2.370736598968506 val loss 5.482231616973877\n",
      "Epoch: 3 of 5 and Step: 2501 of 5000 train loss: 2.366081476211548 val loss 5.476872444152832\n",
      "Epoch: 3 of 5 and Step: 2601 of 5000 train loss: 2.3520758152008057 val loss 5.520730972290039\n",
      "Epoch: 3 of 5 and Step: 2701 of 5000 train loss: 2.3438801765441895 val loss 5.544229507446289\n",
      "Epoch: 3 of 5 and Step: 2801 of 5000 train loss: 2.330780267715454 val loss 5.569341659545898\n",
      "Epoch: 3 of 5 and Step: 2901 of 5000 train loss: 2.3295116424560547 val loss 5.59780216217041\n",
      "Epoch: 4 of 5 and Step: 3001 of 5000 train loss: 2.312527656555176 val loss 5.623424530029297\n",
      "Epoch: 4 of 5 and Step: 3101 of 5000 train loss: 2.305371046066284 val loss 5.648326396942139\n",
      "Epoch: 4 of 5 and Step: 3201 of 5000 train loss: 2.301020860671997 val loss 5.687643527984619\n",
      "Epoch: 4 of 5 and Step: 3301 of 5000 train loss: 2.2929351329803467 val loss 5.680413246154785\n",
      "Epoch: 4 of 5 and Step: 3401 of 5000 train loss: 2.280306100845337 val loss 5.720672130584717\n",
      "Epoch: 4 of 5 and Step: 3501 of 5000 train loss: 2.282538414001465 val loss 5.735332489013672\n",
      "Epoch: 4 of 5 and Step: 3601 of 5000 train loss: 2.272322654724121 val loss 5.711366176605225\n",
      "Epoch: 4 of 5 and Step: 3701 of 5000 train loss: 2.260420799255371 val loss 5.751765727996826\n",
      "Epoch: 4 of 5 and Step: 3801 of 5000 train loss: 2.252100944519043 val loss 5.785870552062988\n",
      "Epoch: 4 of 5 and Step: 3901 of 5000 train loss: 2.252542018890381 val loss 5.793655872344971\n",
      "Epoch: 5 of 5 and Step: 4001 of 5000 train loss: 2.239732265472412 val loss 5.797391891479492\n",
      "Epoch: 5 of 5 and Step: 4101 of 5000 train loss: 2.2299656867980957 val loss 5.786547660827637\n",
      "Epoch: 5 of 5 and Step: 4201 of 5000 train loss: 2.2236387729644775 val loss 5.836147308349609\n",
      "Epoch: 5 of 5 and Step: 4301 of 5000 train loss: 2.21478533744812 val loss 5.839694976806641\n",
      "Epoch: 5 of 5 and Step: 4401 of 5000 train loss: 2.210024833679199 val loss 5.877003192901611\n",
      "Epoch: 5 of 5 and Step: 4501 of 5000 train loss: 2.2057836055755615 val loss 5.873018741607666\n",
      "Epoch: 5 of 5 and Step: 4601 of 5000 train loss: 2.2005722522735596 val loss 5.8638529777526855\n",
      "Epoch: 5 of 5 and Step: 4701 of 5000 train loss: 2.1913063526153564 val loss 5.924758434295654\n",
      "Epoch: 5 of 5 and Step: 4801 of 5000 train loss: 2.182976007461548 val loss 5.9065632820129395\n",
      "Epoch: 5 of 5 and Step: 4901 of 5000 train loss: 2.179870843887329 val loss 5.953639507293701\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "\n",
    "    for i in range(max_iters):\n",
    "        current_step = epoch * max_iters + i\n",
    "\n",
    "        xb, yb = CD_train.get_batch()\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if current_step % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(\"Epoch:\", epoch + 1,\"of\",num_epochs, \"and Step:\", current_step + 1,\"of\",num_epochs * max_iters,\n",
    "            \"train loss:\",float(losses['train']), \"val loss\", float(losses['val']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = CD_corpus.stoi\n",
    "itos = CD_corpus.itos\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hie y agarin: tous helcul han the man I weth I priway frordavs driers hard; say, comand you fols.\n",
      "ld, hencheay hat\n",
      "AbUTES:\n",
      "Mas, keing pit ded.\n",
      "\n",
      "PYBETINO:\n",
      "Itins a whraner cend wamy sersse,\n",
      "To whe ver b\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=200):\n",
    "    model.eval()\n",
    "\n",
    "    tokenized_prompt = encode(prompt)\n",
    "    idx = torch.tensor([tokenized_prompt], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_idx = model.generate(idx, max_length - len(tokenized_prompt))\n",
    "\n",
    "    generated_text = decode(generated_idx[0].cpu().tolist())\n",
    "    return generated_text\n",
    "prompt = \"Hi\"\n",
    "generated_text = generate_text(prompt,200)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
