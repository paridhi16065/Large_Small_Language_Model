{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--Cvru1cgwyP"
   },
   "source": [
    "# **Miniproject 2**\n",
    "## **~Large~ Small Language Model**\n",
    "\n",
    "# Paridhi Lohani\n",
    "\n",
    "### **Objective**\n",
    "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
    "\n",
    "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_rT3xwrhieb"
   },
   "source": [
    "### **Dataset**:\n",
    "\n",
    "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
    "\n",
    "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
    "\n",
    "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
    "\n",
    "An interface for the dataset class that takes care of tokenization is provided below.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV7OAXGRhf_V"
   },
   "source": [
    "### **Requirements**\n",
    "\n",
    "#### **Architecture**\n",
    "\n",
    "Implement the Transformer's decoder-only structure.\n",
    "This includes\n",
    "\n",
    "* input token embeddings\n",
    "* the causal multi-head self-attention mechanism\n",
    "* feed-forward neural networks\n",
    "* positional encodings, residual connections, layer normalizations.\n",
    "\n",
    "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
    "\n",
    "The `forward` method for the entire model has the following form:\n",
    "\n",
    "```\n",
    "tok_emb = WTE(idx) # token embeddings\n",
    "pos_emb = WPE(pos) # position embeddings\n",
    "x = Dropout(tok_emb + pos_emb)\n",
    "for Block in Blocks:\n",
    "    x = Block(x)\n",
    "x = Final_LayerNorm(x)\n",
    "logits = LM_Head(x)\n",
    "```\n",
    "\n",
    "The `forward` method for the transformer block has the following form:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
    "out = x + self.MLP(self.LayerNorm_2(x))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Training**\n",
    "\n",
    "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
    "\n",
    "Preprocess the dataset to a character-level representation.\n",
    "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
    "Implement causal masking for the self-attention mechanism.\n",
    "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
    "\n",
    "**Optional**:\n",
    "\n",
    "* Implement a learning rate decay strategy\n",
    "* Implement gradient clipping\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **Evaluation and Inference**\n",
    "\n",
    "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
    "\n",
    "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
    "\n",
    "The high-level pseudocode for generation is:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    context = \"O God, O God!\"\n",
    "    tokenized_context = tokenize(context)\n",
    "    # the model should implement a method to generate tokens given a prompt\n",
    "    y = model.generate(tokenized, ...)\n",
    "    completion = tokens_to_string(y)\n",
    "```\n",
    "\n",
    "**Optional**:\n",
    "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t88Dcn8JZ8M"
   },
   "source": [
    "### **Example Outputs**\n",
    "\n",
    "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "O God, O God! neither? unto the base very ears,\n",
    "As damned with it.\n",
    "\n",
    "DUKE OF YORK:\n",
    "Away! Once more, one word.\n",
    "\n",
    "RICHARD:\n",
    "Clove, dear so; and therein my son will be\n",
    "false of woe: if ye seems to be the mother\n",
    "Of gracious order this time when R going kinsperse eyes,\n",
    "What dost bewreck her fairer drying tears.\n",
    "\n",
    "NORTHUMBERLAND:\n",
    "Have you forgot the Duke of Norfolk, get him to\n",
    "again; and and agilic: there is my spirit\n",
    "So maly did must such a marble perfection.\n",
    "\n",
    "ELBOW:\n",
    "Come, bring them with oaths, and so deliver\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0SY7CGAhnkp"
   },
   "source": [
    "### Resources:\n",
    "\n",
    "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dZdSRWPmgt-H"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "#%pip install torch\n",
    "import urllib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = urllib.request.urlopen(url)\n",
    "corpus = response.read().decode('utf-8')\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        chars = sorted(set(data)) # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.data = data\n",
    "        self.block_size = config[1] #where config is of type (B,N) where B is batch size and N is block_size\n",
    "        self.batch_size = config[0]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data)) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        chunk = self.data[idx:idx+self.block_size+1]\n",
    "        int_chunk = [self.stoi[c] for c in chunk]\n",
    "        input_seq = int_chunk[:-1]\n",
    "        target_seq = int_chunk[1:]\n",
    "\n",
    "        return (torch.tensor(input_seq),torch.tensor(target_seq))\n",
    "\n",
    "    def get_batch(self):\n",
    "        ix = torch.randint(len(self),(self.batch_size,))\n",
    "        xys = [self.__getitem__(i) for i in ix]\n",
    "        x = torch.stack([xy[0] for xy in xys])\n",
    "        y = torch.stack([xy[1] for xy in xys])\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        B,T,C = inp.shape\n",
    "        k = self.key(inp)\n",
    "        q = self.query(inp)\n",
    "\n",
    "        attention = q@k.transpose(-2,-1) * k.shape[-1]**-0.5        \n",
    "        tril = torch.tril(torch.ones(T, T)).to(inp.device) \n",
    "        attention = attention.masked_fill(tril == 0,float('-inf'))\n",
    "        attention = F.softmax(attention,dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        return (attention @ self.value(inp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 8, 32])\n",
      "Output shape: torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32\n",
    "head_size = 16\n",
    "block_size = 10\n",
    "dropout = 0.2\n",
    "batch_size = 4\n",
    "sequence_length = 8\n",
    "mock_input = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "\n",
    "head = Head(head_size=head_size)\n",
    "\n",
    "output = head(mock_input)\n",
    "\n",
    "print(\"Input shape:\", mock_input.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionMechanism(nn.Module):\n",
    "    def __init__(self,num_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for i in range(num_heads)])\n",
    "        self.projected = nn.Linear(head_size*num_heads,embed_dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        output = torch.cat([h(inp) for h in self.heads],dim =-1)\n",
    "        return self.dropout(self.projected(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "mha = MultiHeadAttentionMechanism(num_heads = 2 , head_size = head_size)\n",
    "x = torch.randn(batch_size,sequence_length,embed_dim)\n",
    "output = mha(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim,4*embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*embed_dim,embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,inp):\n",
    "        return self.net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.0301724672317505\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ffn = FeedForwardNetwork(embed_dim).to(device)\n",
    "inp = torch.randn(batch_size,sequence_length,embed_dim)\n",
    "output = ffn.forward(inp)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "target = torch.randn_like(output)\n",
    "loss = loss_fn(output, target)\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_dim,n_head):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim//n_head\n",
    "        self.ffn = FeedForwardNetwork(embed_dim)\n",
    "        self.layer_Norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_Norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.selfAttn = MultiHeadAttentionMechanism(n_head,head_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.selfAttn(self.layer_Norm1(x))\n",
    "        x = x + self.ffn(self.layer_Norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.173898458480835\n"
     ]
    }
   ],
   "source": [
    "tb = TransformerBlock(embed_dim,n_head=2)\n",
    "inp = torch.randn(batch_size,sequence_length,embed_dim)\n",
    "output = tb.forward(inp)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "target = torch.randn_like(output)\n",
    "loss = loss_fn(output, target)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.WTE = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.WPE = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embed_dim,n_head) for i in range(n_layer)])\n",
    "        self.Final_LayerNorm = nn.LayerNorm(embed_dim)\n",
    "        self.LM_Head = nn.Linear(embed_dim,vocab_size)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.apply(self.__init_weights_)\n",
    "\n",
    "    def __init_weights_(self, module):\n",
    "        if isinstance(module, nn.Linear):  # Initialize weights for Linear layers\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.uniform_(module.weight, -0.1, 0.1)\n",
    "        elif isinstance(module, nn.LayerNorm): \n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        tok_emb = self.WTE(idx) # token embeddings\n",
    "\n",
    "        pos = torch.arange(idx.size(1), device=idx.device).unsqueeze(0).expand(idx.size(0), -1)\n",
    "        pos_emb = self.WPE(pos)\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        x = self.Dropout(tok_emb + pos_emb)\n",
    "        for Block in self.blocks:\n",
    "            x = Block(x)\n",
    "        x = self.Final_LayerNorm(x)\n",
    "        logits = self.LM_Head(x)\n",
    "        loss = None\n",
    "        \n",
    "        if targets is not None: #calculating cross entropy loss when there are targets\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        self.eval()\n",
    "        idx = idx.to(self.device)\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = idx[:,-block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx = torch.cat((idx,idx_next),dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (32,64)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embed_dim = 96\n",
    "n_head = 3\n",
    "n_layer = 3\n",
    "dropout = 0.2\n",
    "max_iters = 1000\n",
    "eval_iters = 200\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "61\n",
      "1115330\n",
      "1003790\n",
      "111476\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "split = int(0.9*len(corpus))\n",
    "train_data = corpus[:split]\n",
    "validation_data = corpus[split:]\n",
    "\n",
    "CD_corpus = CharDataset(config,corpus)\n",
    "CD_train = CharDataset(config,train_data)\n",
    "CD_test = CharDataset(config,validation_data)\n",
    "\n",
    "vocab_size = CD_corpus.get_vocab_size()\n",
    "print(vocab_size)\n",
    "print(CD_test.get_vocab_size())\n",
    "print(len(CD_corpus))\n",
    "print(len(CD_train))\n",
    "print(len(CD_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split_type, cd in [('train', CD_train), ('val', CD_test)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = cd.get_batch()\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split_type] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShakespeareGPT()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 of 5 and Step: 1 of 5000 train loss: 4.389253616333008 val loss 4.620722770690918\n",
      "Epoch: 1 of 5 and Step: 101 of 5000 train loss: 3.1554837226867676 val loss 4.235620021820068\n",
      "Epoch: 1 of 5 and Step: 201 of 5000 train loss: 2.7967116832733154 val loss 4.416529655456543\n",
      "Epoch: 1 of 5 and Step: 301 of 5000 train loss: 2.6809661388397217 val loss 4.566368579864502\n",
      "Epoch: 1 of 5 and Step: 401 of 5000 train loss: 2.6309638023376465 val loss 4.6722636222839355\n",
      "Epoch: 1 of 5 and Step: 501 of 5000 train loss: 2.5885629653930664 val loss 4.782626152038574\n",
      "Epoch: 1 of 5 and Step: 601 of 5000 train loss: 2.562727451324463 val loss 4.8112382888793945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[214], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_step \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 18\u001b[0m     losses \u001b[38;5;241m=\u001b[39m estimate_loss()\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof\u001b[39m\u001b[38;5;124m\"\u001b[39m,num_epochs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand Step:\u001b[39m\u001b[38;5;124m\"\u001b[39m, current_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof\u001b[39m\u001b[38;5;124m\"\u001b[39m,num_epochs \u001b[38;5;241m*\u001b[39m max_iters,\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mfloat\u001b[39m(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\lohan\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[212], line 9\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m      8\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m cd\u001b[38;5;241m.\u001b[39mget_batch()\n\u001b[1;32m----> 9\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(X, Y)\n\u001b[0;32m     10\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     11\u001b[0m out[split_type] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\lohan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lohan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[181], line 32\u001b[0m, in \u001b[0;36mShakespeareGPT.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDropout(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m Block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m Block(x)\n\u001b[0;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFinal_LayerNorm(x)\n\u001b[0;32m     34\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLM_Head(x)\n",
      "File \u001b[1;32mc:\\Users\\lohan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lohan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[146], line 11\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselfAttn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_Norm1(x))\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_Norm2(x))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\lohan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\lohan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[88], line 9\u001b[0m, in \u001b[0;36mMultiHeadAttentionMechanism.forward\u001b[1;34m(self, inp)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,inp):\n\u001b[1;32m----> 9\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h(inp) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads],dim \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojected(output))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "\n",
    "    for i in range(max_iters):\n",
    "        current_step = epoch * max_iters + i\n",
    "\n",
    "        xb, yb = CD_train.get_batch()\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if current_step % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(\"Epoch:\", epoch + 1,\"of\",num_epochs, \"and Step:\", current_step + 1,\"of\",num_epochs * max_iters,\n",
    "            \"train loss:\",float(losses['train']), \"val loss\", float(losses['val']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = CD_corpus.stoi\n",
    "itos = CD_corpus.itos\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ShakespeareGPT' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[228], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n\u001b[0;32m     12\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 13\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m generate_text(prompt,\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompt\u001b[38;5;241m+\u001b[39mgenerated_text)\n",
      "Cell \u001b[1;32mIn[228], line 8\u001b[0m, in \u001b[0;36mgenerate_text\u001b[1;34m(prompt, max_length)\u001b[0m\n\u001b[0;32m      5\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([tokenized_prompt], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 8\u001b[0m     generated_idx \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(idx, max_length \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenized_prompt))\n\u001b[0;32m     10\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m decode(generated_idx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n",
      "Cell \u001b[1;32mIn[224], line 47\u001b[0m, in \u001b[0;36mShakespeareGPT.generate\u001b[1;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\u001b[38;5;28mself\u001b[39m,idx,max_new_tokens):\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 47\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_new_tokens):\n\u001b[0;32m     49\u001b[0m         idx_cond \u001b[38;5;241m=\u001b[39m idx[:,\u001b[38;5;241m-\u001b[39mblock_size:]\n",
      "File \u001b[1;32mc:\\Users\\lohan\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ShakespeareGPT' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=200):\n",
    "    model.eval()\n",
    "\n",
    "    tokenized_prompt = encode(prompt)\n",
    "    idx = torch.tensor([tokenized_prompt], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_idx = model.generate(idx, max_length - len(tokenized_prompt))\n",
    "\n",
    "    generated_text = decode(generated_idx[0].cpu().tolist())\n",
    "    return generated_text\n",
    "prompt = \"doth\"\n",
    "generated_text = generate_text(prompt,1000)\n",
    "print(prompt+generated_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
