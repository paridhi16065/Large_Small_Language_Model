{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--Cvru1cgwyP"
   },
   "source": [
    "# **Miniproject 2**\n",
    "## **~Large~ Small Language Model**\n",
    "\n",
    "# Paridhi Lohani\n",
    "\n",
    "### **Objective**\n",
    "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
    "\n",
    "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_rT3xwrhieb"
   },
   "source": [
    "### **Dataset**:\n",
    "\n",
    "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
    "\n",
    "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
    "\n",
    "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
    "\n",
    "An interface for the dataset class that takes care of tokenization is provided below.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV7OAXGRhf_V"
   },
   "source": [
    "### **Requirements**\n",
    "\n",
    "#### **Architecture**\n",
    "\n",
    "Implement the Transformer's decoder-only structure.\n",
    "This includes\n",
    "\n",
    "* input token embeddings\n",
    "* the causal multi-head self-attention mechanism\n",
    "* feed-forward neural networks\n",
    "* positional encodings, residual connections, layer normalizations.\n",
    "\n",
    "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
    "\n",
    "The `forward` method for the entire model has the following form:\n",
    "\n",
    "```\n",
    "tok_emb = WTE(idx) # token embeddings\n",
    "pos_emb = WPE(pos) # position embeddings\n",
    "x = Dropout(tok_emb + pos_emb)\n",
    "for Block in Blocks:\n",
    "    x = Block(x)\n",
    "x = Final_LayerNorm(x)\n",
    "logits = LM_Head(x)\n",
    "```\n",
    "\n",
    "The `forward` method for the transformer block has the following form:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
    "out = x + self.MLP(self.LayerNorm_2(x))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Training**\n",
    "\n",
    "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
    "\n",
    "Preprocess the dataset to a character-level representation.\n",
    "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
    "Implement causal masking for the self-attention mechanism.\n",
    "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
    "\n",
    "**Optional**:\n",
    "\n",
    "* Implement a learning rate decay strategy\n",
    "* Implement gradient clipping\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **Evaluation and Inference**\n",
    "\n",
    "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
    "\n",
    "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
    "\n",
    "The high-level pseudocode for generation is:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    context = \"O God, O God!\"\n",
    "    tokenized_context = tokenize(context)\n",
    "    # the model should implement a method to generate tokens given a prompt\n",
    "    y = model.generate(tokenized, ...)\n",
    "    completion = tokens_to_string(y)\n",
    "```\n",
    "\n",
    "**Optional**:\n",
    "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t88Dcn8JZ8M"
   },
   "source": [
    "### **Example Outputs**\n",
    "\n",
    "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "O God, O God! neither? unto the base very ears,\n",
    "As damned with it.\n",
    "\n",
    "DUKE OF YORK:\n",
    "Away! Once more, one word.\n",
    "\n",
    "RICHARD:\n",
    "Clove, dear so; and therein my son will be\n",
    "false of woe: if ye seems to be the mother\n",
    "Of gracious order this time when R going kinsperse eyes,\n",
    "What dost bewreck her fairer drying tears.\n",
    "\n",
    "NORTHUMBERLAND:\n",
    "Have you forgot the Duke of Norfolk, get him to\n",
    "again; and and agilic: there is my spirit\n",
    "So maly did must such a marble perfection.\n",
    "\n",
    "ELBOW:\n",
    "Come, bring them with oaths, and so deliver\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0SY7CGAhnkp"
   },
   "source": [
    "### Resources:\n",
    "\n",
    "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dZdSRWPmgt-H"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "#%pip install torch\n",
    "import urllib\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = urllib.request.urlopen(url)\n",
    "corpus = response.read().decode('utf-8')\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "        chars = sorted(set(data)) # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.data = data\n",
    "        self.block_size = config[1] #where config is of type (B,N) where B is batch size and N is block_size\n",
    "        self.batch_size = config[0]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.stoi)\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.data)) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        chunk = self.data[idx:idx+self.block_size+1]\n",
    "        int_chunk = [self.stoi[c] for c in chunk]\n",
    "        input_seq = int_chunk[:-1]\n",
    "        target_seq = int_chunk[1:]\n",
    "\n",
    "        return (torch.tensor(input_seq),torch.tensor(target_seq))\n",
    "\n",
    "    def get_batch(self):\n",
    "        ix = torch.randint(len(self),(self.batch_size,))\n",
    "        xys = [self.__getitem__(i) for i in ix]\n",
    "        x = torch.stack([xy[0] for xy in xys])\n",
    "        y = torch.stack([xy[1] for xy in xys])\n",
    "        x,y = x.to(device),y.to(device)\n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self,head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.query = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.value = nn.Linear(embed_dim, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        B,T,C = inp.shape\n",
    "        k = self.key(inp)\n",
    "        q = self.query(inp)\n",
    "\n",
    "        attention = q@k.transpose(-2,-1) * k.shape[-1]**-0.5        \n",
    "        tril = torch.tril(torch.ones(T, T)).to(inp.device) \n",
    "        attention = attention.masked_fill(tril == 0,float('-inf'))\n",
    "        attention = F.softmax(attention,dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        return (attention @ self.value(inp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 8, 32])\n",
      "Output shape: torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32\n",
    "head_size = 16\n",
    "block_size = 10\n",
    "dropout = 0.2\n",
    "batch_size = 4\n",
    "sequence_length = 8\n",
    "mock_input = torch.randn(batch_size, sequence_length, embed_dim)\n",
    "\n",
    "head = Head(head_size=head_size)\n",
    "\n",
    "output = head(mock_input)\n",
    "\n",
    "print(\"Input shape:\", mock_input.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionMechanism(nn.Module):\n",
    "    def __init__(self,num_heads,head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for i in range(num_heads)])\n",
    "        self.projected = nn.Linear(head_size*num_heads,embed_dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        output = torch.cat([h(inp) for h in self.heads],dim =-1)\n",
    "        return self.dropout(self.projected(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.1\n",
    "mha = MultiHeadAttentionMechanism(num_heads = 2 , head_size = head_size)\n",
    "x = torch.randn(batch_size,sequence_length,embed_dim)\n",
    "output = mha(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim,4*embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*embed_dim,embed_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self,inp):\n",
    "        return self.net(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.008808970451355\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ffn = FeedForwardNetwork(embed_dim).to(device)\n",
    "inp = torch.randn(batch_size,sequence_length,embed_dim)\n",
    "output = ffn.forward(inp)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "target = torch.randn_like(output)\n",
    "loss = loss_fn(output, target)\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_dim,n_head):\n",
    "        super().__init__()\n",
    "        head_size = embed_dim//n_head\n",
    "        self.ffn = FeedForwardNetwork(embed_dim)\n",
    "        self.layer_Norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_Norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.selfAttn = MultiHeadAttentionMechanism(n_head,head_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.selfAttn(self.layer_Norm1(x))\n",
    "        x = x + self.ffn(self.layer_Norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.0736894607543945\n"
     ]
    }
   ],
   "source": [
    "tb = TransformerBlock(embed_dim,n_head=2)\n",
    "inp = torch.randn(batch_size,sequence_length,embed_dim)\n",
    "output = tb.forward(inp)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "target = torch.randn_like(output)\n",
    "loss = loss_fn(output, target)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.WTE = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.WPE = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embed_dim,n_head) for i in range(n_layer)])\n",
    "        self.Final_LayerNorm = nn.LayerNorm(embed_dim)\n",
    "        self.LM_Head = nn.Linear(embed_dim,vocab_size)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.apply(self.__init_weights_)\n",
    "\n",
    "    def __init_weights_(self, module):\n",
    "        if isinstance(module, nn.Linear):  # Initialize weights for Linear layers\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.uniform_(module.weight, -0.1, 0.1)\n",
    "        elif isinstance(module, nn.LayerNorm): \n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "            \n",
    "\n",
    "    def forward(self,idx,targets=None):\n",
    "        tok_emb = self.WTE(idx) # token embeddings\n",
    "\n",
    "        pos = torch.arange(idx.size(1), device=idx.device).unsqueeze(0).expand(idx.size(0), -1)\n",
    "        pos_emb = self.WPE(pos)\n",
    "        self.Dropout = nn.Dropout(dropout)\n",
    "        x = self.Dropout(tok_emb + pos_emb)\n",
    "        for Block in self.blocks:\n",
    "            x = Block(x)\n",
    "        x = self.Final_LayerNorm(x)\n",
    "        logits = self.LM_Head(x)\n",
    "        loss = None\n",
    "        \n",
    "        if targets is not None: #calculating cross entropy loss when there are targets\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        self.eval()\n",
    "        idx = idx.to(self.device)\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = idx[:,-block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits,dim=-1)\n",
    "            idx_next = torch.multinomial(probs,num_samples=1)\n",
    "            idx = torch.cat((idx,idx_next),dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (32,64)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "embed_dim = 96\n",
    "n_head = 3\n",
    "n_layer = 3\n",
    "dropout = 0.3\n",
    "max_iters = 1000\n",
    "eval_iters = 200\n",
    "eval_interval = 100\n",
    "learning_rate = 3e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "61\n",
      "1115330\n",
      "1003790\n",
      "111476\n"
     ]
    }
   ],
   "source": [
    "#train test split\n",
    "split = int(0.9*len(corpus))\n",
    "train_data = corpus[:split]\n",
    "validation_data = corpus[split:]\n",
    "\n",
    "CD_corpus = CharDataset(config,corpus)\n",
    "CD_train = CharDataset(config,train_data)\n",
    "CD_test = CharDataset(config,validation_data)\n",
    "\n",
    "vocab_size = CD_corpus.get_vocab_size()\n",
    "print(vocab_size)\n",
    "print(CD_test.get_vocab_size())\n",
    "print(len(CD_corpus))\n",
    "print(len(CD_train))\n",
    "print(len(CD_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split_type, cd in [('train', CD_train), ('val', CD_test)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = cd.get_batch()\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split_type] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShakespeareGPT()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 of 5 and Step: 1 of 5000 train loss: 4.372241973876953 val loss 4.554647445678711\n",
      "Epoch: 1 of 5 and Step: 101 of 5000 train loss: 3.275624990463257 val loss 4.258957862854004\n",
      "Epoch: 1 of 5 and Step: 201 of 5000 train loss: 2.948925256729126 val loss 4.351941108703613\n",
      "Epoch: 1 of 5 and Step: 301 of 5000 train loss: 2.750532865524292 val loss 4.4615478515625\n",
      "Epoch: 1 of 5 and Step: 401 of 5000 train loss: 2.6819851398468018 val loss 4.580870151519775\n",
      "Epoch: 1 of 5 and Step: 501 of 5000 train loss: 2.636129140853882 val loss 4.656436443328857\n",
      "Epoch: 1 of 5 and Step: 601 of 5000 train loss: 2.5991480350494385 val loss 4.7820563316345215\n",
      "Epoch: 1 of 5 and Step: 701 of 5000 train loss: 2.576502799987793 val loss 4.841875076293945\n",
      "Epoch: 1 of 5 and Step: 801 of 5000 train loss: 2.5563981533050537 val loss 4.9320149421691895\n",
      "Epoch: 1 of 5 and Step: 901 of 5000 train loss: 2.5392777919769287 val loss 4.968745708465576\n",
      "Epoch: 2 of 5 and Step: 1001 of 5000 train loss: 2.5261802673339844 val loss 5.066069602966309\n",
      "Epoch: 2 of 5 and Step: 1101 of 5000 train loss: 2.511538028717041 val loss 5.114107608795166\n",
      "Epoch: 2 of 5 and Step: 1201 of 5000 train loss: 2.492544174194336 val loss 5.159042835235596\n",
      "Epoch: 2 of 5 and Step: 1301 of 5000 train loss: 2.4929423332214355 val loss 5.181941032409668\n",
      "Epoch: 2 of 5 and Step: 1401 of 5000 train loss: 2.4783952236175537 val loss 5.236424446105957\n",
      "Epoch: 2 of 5 and Step: 1501 of 5000 train loss: 2.472578525543213 val loss 5.261980056762695\n",
      "Epoch: 2 of 5 and Step: 1601 of 5000 train loss: 2.4573616981506348 val loss 5.280400276184082\n",
      "Epoch: 2 of 5 and Step: 1701 of 5000 train loss: 2.4448914527893066 val loss 5.345529556274414\n",
      "Epoch: 2 of 5 and Step: 1801 of 5000 train loss: 2.44294810295105 val loss 5.360639572143555\n",
      "Epoch: 2 of 5 and Step: 1901 of 5000 train loss: 2.428497076034546 val loss 5.397113800048828\n",
      "Epoch: 3 of 5 and Step: 2001 of 5000 train loss: 2.4251163005828857 val loss 5.433588981628418\n",
      "Epoch: 3 of 5 and Step: 2101 of 5000 train loss: 2.4150476455688477 val loss 5.445705413818359\n",
      "Epoch: 3 of 5 and Step: 2201 of 5000 train loss: 2.4057347774505615 val loss 5.4470438957214355\n",
      "Epoch: 3 of 5 and Step: 2301 of 5000 train loss: 2.39975905418396 val loss 5.47313117980957\n",
      "Epoch: 3 of 5 and Step: 2401 of 5000 train loss: 2.383572816848755 val loss 5.510503768920898\n",
      "Epoch: 3 of 5 and Step: 2501 of 5000 train loss: 2.3723502159118652 val loss 5.5199294090271\n",
      "Epoch: 3 of 5 and Step: 2601 of 5000 train loss: 2.365468740463257 val loss 5.56535530090332\n",
      "Epoch: 3 of 5 and Step: 2701 of 5000 train loss: 2.356433153152466 val loss 5.549383163452148\n",
      "Epoch: 3 of 5 and Step: 2801 of 5000 train loss: 2.348745346069336 val loss 5.620060443878174\n",
      "Epoch: 3 of 5 and Step: 2901 of 5000 train loss: 2.3356146812438965 val loss 5.601390838623047\n",
      "Epoch: 4 of 5 and Step: 3001 of 5000 train loss: 2.325507164001465 val loss 5.643404006958008\n",
      "Epoch: 4 of 5 and Step: 3101 of 5000 train loss: 2.317544460296631 val loss 5.641275405883789\n",
      "Epoch: 4 of 5 and Step: 3201 of 5000 train loss: 2.313856363296509 val loss 5.720885753631592\n",
      "Epoch: 4 of 5 and Step: 3301 of 5000 train loss: 2.3008549213409424 val loss 5.682241439819336\n",
      "Epoch: 4 of 5 and Step: 3401 of 5000 train loss: 2.288778305053711 val loss 5.692625522613525\n",
      "Epoch: 4 of 5 and Step: 3501 of 5000 train loss: 2.2846198081970215 val loss 5.731379508972168\n",
      "Epoch: 4 of 5 and Step: 3601 of 5000 train loss: 2.273209571838379 val loss 5.732903957366943\n",
      "Epoch: 4 of 5 and Step: 3701 of 5000 train loss: 2.2580745220184326 val loss 5.7698774337768555\n",
      "Epoch: 4 of 5 and Step: 3801 of 5000 train loss: 2.2594847679138184 val loss 5.7699480056762695\n",
      "Epoch: 4 of 5 and Step: 3901 of 5000 train loss: 2.2470543384552 val loss 5.816018104553223\n",
      "Epoch: 5 of 5 and Step: 4001 of 5000 train loss: 2.243328809738159 val loss 5.812231063842773\n",
      "Epoch: 5 of 5 and Step: 4101 of 5000 train loss: 2.234234571456909 val loss 5.854165077209473\n",
      "Epoch: 5 of 5 and Step: 4201 of 5000 train loss: 2.230623245239258 val loss 5.853207588195801\n",
      "Epoch: 5 of 5 and Step: 4301 of 5000 train loss: 2.2210593223571777 val loss 5.895284652709961\n",
      "Epoch: 5 of 5 and Step: 4401 of 5000 train loss: 2.218923807144165 val loss 5.893818378448486\n",
      "Epoch: 5 of 5 and Step: 4501 of 5000 train loss: 2.2088916301727295 val loss 5.925286769866943\n",
      "Epoch: 5 of 5 and Step: 4601 of 5000 train loss: 2.196295976638794 val loss 5.914868354797363\n",
      "Epoch: 5 of 5 and Step: 4701 of 5000 train loss: 2.193772792816162 val loss 5.909824848175049\n",
      "Epoch: 5 of 5 and Step: 4801 of 5000 train loss: 2.1859662532806396 val loss 5.902749538421631\n",
      "Epoch: 5 of 5 and Step: 4901 of 5000 train loss: 2.1841652393341064 val loss 5.962550640106201\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 5 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "\n",
    "    for i in range(max_iters):\n",
    "        current_step = epoch * max_iters + i\n",
    "\n",
    "        xb, yb = CD_train.get_batch()\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if current_step % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            print(\"Epoch:\", epoch + 1,\"of\",num_epochs, \"and Step:\", current_step + 1,\"of\",num_epochs * max_iters,\n",
    "            \"train loss:\",float(losses['train']), \"val loss\", float(losses['val']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = CD_corpus.stoi\n",
    "itos = CD_corpus.itos\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hicht be and arevends to, itld\n",
      "And he mues stht elivid this.\n",
      "PERY:\n",
      "Nors\n",
      "Ay po'd Furackild onth he to by.\n",
      "LOFHANIN:\n",
      "Los of thie, ave mod der that we\n",
      "Pom's mowend!\n",
      "\n",
      "\n",
      "LONINI I:\n",
      "Oshat my aros, recate;\n",
      "Sol\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, max_length=200):\n",
    "    model.eval()\n",
    "\n",
    "    tokenized_prompt = encode(prompt)\n",
    "    idx = torch.tensor([tokenized_prompt], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_idx = model.generate(idx, max_length - len(tokenized_prompt))\n",
    "\n",
    "    generated_text = decode(generated_idx[0].cpu().tolist())\n",
    "    return generated_text\n",
    "prompt = \"Hi\"\n",
    "generated_text = generate_text(prompt,200)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
